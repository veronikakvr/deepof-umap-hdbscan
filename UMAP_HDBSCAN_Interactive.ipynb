{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Interactive UMAP & HDBSCAN Clustering Pipeline\n",
        "\n",
        "This notebook allows you to run the clustering pipeline step-by-step with adjustable parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import umap\n",
        "import hdbscan\n",
        "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data\n",
        "Set your CSV file paths below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EDIT PATHS HERE\n",
        "female_csv_path = 'path/to/females.csv'\n",
        "male_csv_path = 'path/to/males.csv'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define pipeline functions (same as in the script, you can modularize or import externally)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paste or import functions from the script here for all the modular steps\n",
        "# For brevity, please copy the functions `load_and_combine_data`, `create_intervals`, `aggregate_behaviors`,\n",
        "# `preprocess_and_impute`, `scale_features`, `compute_umap_embedding`, `perform_hdbscan_clustering`, `add_cluster_labels`, and `plot_umap_clusters`\n",
        "# exactly as defined in the above script cell into separate notebook cells or a single cell.\n",
        "\n",
        "# ... example for loading and combining data function ...\n",
        "def load_and_combine_data(female_path, male_path):\n",
        "    df_females = pd.read_csv(female_path)\n",
        "    df_males = pd.read_csv(male_path)\n",
        "    df_females['Sex'] = 'Female'\n",
        "    df_males['Sex'] = 'Male'\n",
        "    combined_df = pd.concat([df_females, df_males], ignore_index=True)\n",
        "    if not np.issubdtype(combined_df['Time'].dtype, np.timedelta64):\n",
        "        combined_df['Time'] = pd.to_timedelta(combined_df['Time'])\n",
        "    return combined_df\n",
        "\n",
        "# (Continue copying all other functions here similarly)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameters\n",
        "Adjust these parameters before running the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "interval_seconds = 2\n",
        "exclude_ids = ['ID63', 'ID214']\n",
        "exclude_geno = 'atg7OE'\n",
        "last_interval_label = \"0 days 00:09:58 - 0 days 00:10:00\"\n",
        "\n",
        "umap_params = {\n",
        "    'n_components': 2,\n",
        "    'n_neighbors': 25,\n",
        "    'min_dist': 0.1,\n",
        "    'metric': 'euclidean',\n",
        "    'random_state': 42,\n",
        "    'verbose': True\n",
        "}\n",
        "\n",
        "hdbscan_params = {\n",
        "    'min_cluster_size': 500,\n",
        "    'min_samples': 90\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the Clustering Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run pipeline step-by-step\n",
        "combined_df = load_and_combine_data(female_csv_path, male_csv_path)\n",
        "combined_df = create_intervals(combined_df, interval_seconds)\n",
        "behavior_cols = [\n",
        "    'B_W_nose2nose', 'B_W_sidebyside', 'B_W_sidereside', 'B_W_nose2tail', 'B_W_nose2body',\n",
        "    'B_W_following', 'B_climb-arena', 'B_sniff-arena', 'B_immobility', 'B_stat-lookaround',\n",
        "    'B_stat-active', 'B_stat-passive', 'B_moving', 'B_sniffing', 'B_speed'\n",
        "]\n",
        "agg_df = aggregate_behaviors(combined_df, behavior_cols)\n",
        "filtered_df, behavior_cols_filtered = preprocess_and_impute(\n",
        "    agg_df, exclude_ids=exclude_ids, exclude_geno=exclude_geno, last_label=last_interval_label\n",
        ")\n",
        "scaled_data, scaler = scale_features(filtered_df, behavior_cols_filtered)\n",
        "embedding, umap_model = compute_umap_embedding(scaled_data, umap_params)\n",
        "labels, hdbscan_model = perform_hdbscan_clustering(embedding, hdbscan_params)\n",
        "filtered_df = add_cluster_labels(filtered_df, labels)\n",
        "\n",
        "# Visualize\n",
        "plot_umap_clusters(embedding, labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_csv_path = 'clustered_results.csv'\n",
        "filtered_df.to_csv(output_csv_path, index=False)\n",
        "print(f'Saved clustered results to {output_csv_path}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

